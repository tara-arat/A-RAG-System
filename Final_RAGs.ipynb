{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "def load_pdf_and_split(path):\n",
        "    pdf_loader = PyPDFLoader(path)\n",
        "    pdf_pages = pdf_loader.load_and_split()\n",
        "    text_chunks = pdf_pages[0].page_content\n",
        "    return text_chunks\n",
        "\n",
        "def split_text_into_chunks(text_chunks, chunk_size):\n",
        "    text_splitter = CharacterTextSplitter(chunk_size=chunk_size)\n",
        "    chunks = text_splitter.create_documents([text_chunks])\n",
        "    return chunks\n",
        "\n",
        "def embed_documents(text_chunks, api_key, model):\n",
        "    embedding_model = GoogleGenerativeAIEmbeddings(google_api_key=api_key, model=model)\n",
        "    vectors = embedding_model.embed_documents(text_chunks)\n",
        "    return vectors\n",
        "\n",
        "def create_vector_store(chunks, embedding_model):\n",
        "    db = Chroma.from_documents(chunks, embedding_model)\n",
        "    db.persist()\n",
        "    return db\n",
        "\n",
        "def create_retriever(db_connection):\n",
        "    retriever = db_connection.as_retriever(search_kwargs={\"k\": 5})\n",
        "    return retriever\n",
        "\n",
        "def build_rag_chain(retriever, chat_template, api_key, model):\n",
        "    output_parser = StrOutputParser()\n",
        "    rag_chain = (\n",
        "        {\"context\": retriever | (lambda docs: \"\\n\\n\".join(doc.page_content for doc in docs)),\n",
        "         \"question\": RunnablePassthrough()}\n",
        "        | chat_template\n",
        "        | (ChatGoogleGenerativeAI(google_api_key=api_key, model=model)\n",
        "          | output_parser)\n",
        "    )\n",
        "    return rag_chain\n",
        "\n",
        "def main():\n",
        "    path = '/content/2404.07143.pdf'\n",
        "    text_chunks = load_pdf_and_split(path)\n",
        "    chunks = split_text_into_chunks(text_chunks, chunk_size=300)\n",
        "    with open(\"/content/api.txt\") as f:\n",
        "        api_key = f.read().strip()\n",
        "    vectors = embed_documents(text_chunks, api_key, model=\"models/embedding-001\")\n",
        "    db = create_vector_store(chunks, GoogleGenerativeAIEmbeddings(google_api_key=api_key, model=\"models/embedding-001\"))\n",
        "    db_connection = Chroma(embedding_function=GoogleGenerativeAIEmbeddings(google_api_key=api_key, model=\"models/embedding-001\"))\n",
        "    retriever = create_retriever(db_connection)\n",
        "    chat_template = ChatPromptTemplate.from_messages([\n",
        "        SystemMessage(content=\"I'm a helpful AI assistant. I'll use the provided document to answer your questions.\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"\"\"Answer the following question based on the provided context:\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "\n",
        "        Answer:\"\"\")\n",
        "    ])\n",
        "    model = \"gemini-1.5-pro-latest\"\n",
        "    rag_chain = build_rag_chain(retriever, chat_template, api_key, model)\n",
        "    user_question = input(\"Enter your question: \")\n",
        "    logging.info(f\"User question: {user_question}\")\n",
        "    response = rag_chain.invoke(user_question)\n",
        "    print(response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpmdM6fcrGpd",
        "outputId": "9dfcc3f7-1433-4e02-94c5-4fde7f3636ba"
      },
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your question: Give the most unique points\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Unique Points of Infini-attention:\n",
            "\n",
            "Based on the provided context, here are the most unique points of Infini-attention:\n",
            "\n",
            "* **Combines Compressive Memory with Attention:** Infini-attention integrates a compressive memory system into the traditional attention mechanism. This allows it to handle infinitely long input sequences while maintaining bounded memory and computational costs. This is a significant departure from standard Transformers, which struggle with memory limitations for long sequences.\n",
            "* **Hybrid Attention Mechanism:**  It utilizes both masked local attention and long-term linear attention within a single Transformer block. This enables the model to capture both local context and long-range dependencies effectively.\n",
            "* **Efficiency and Scalability:** The use of compressive memory allows for efficient processing of long sequences, making it more scalable than traditional attention mechanisms. This efficiency also translates to faster streaming inference for LLMs.\n",
            "* **Minimal Additional Parameters:** Despite its added capabilities, Infini-attention introduces minimal additional parameters compared to standard Transformer architectures. This helps maintain the model's efficiency and avoids excessive complexity.\n",
            "* **Practical Application:** The research demonstrates the effectiveness of Infini-attention on various tasks, including long-context language modeling, passkey context block retrieval, and book summarization. This showcases its practical applicability and potential for real-world use cases. \n",
            "\n",
            "**In summary, Infini-attention presents a unique and promising approach to address the memory and computational challenges of handling long sequences in LLMs. Its hybrid attention mechanism, combined with the use of compressive memory, allows for efficient and scalable processing while maintaining model complexity at a manageable level.** \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "class Document:\n",
        "    def __init__(self, page_content, metadata=None):\n",
        "        self.page_content = page_content\n",
        "        self.metadata = metadata\n",
        "\n",
        "\n",
        "class RAGChain:\n",
        "    def __init__(self, retriever, llm, output_parser, format_docs):\n",
        "        self.retriever = retriever\n",
        "        self.llm = llm\n",
        "        self.output_parser = output_parser\n",
        "        self.format_docs = format_docs\n",
        "\n",
        "    def invoke(self, input_string, retrieved_docs=None):\n",
        "        if retrieved_docs is not None:\n",
        "            docs = [Document(page_content=doc.page_content) for doc in retrieved_docs]\n",
        "        else:\n",
        "            docs = self.retriever.retrieve(input_string)\n",
        "\n",
        "        docs_formatted = self.format_docs(docs)\n",
        "        output = self.llm(docs_formatted)\n",
        "        parsed_output = self.output_parser.parse(output)\n",
        "        return parsed_output\n",
        "\n",
        "pdf_loader = PyPDFLoader('/content/2404.07143.pdf')\n",
        "pdf_pages = pdf_loader.load_and_split()\n",
        "\n",
        "text_chunks = [Document(page_content=page.page_content) for page in pdf_pages]\n",
        "\n",
        "embedding_model = GoogleGenerativeAIEmbeddings(google_api_key=\"____\",\n",
        "                                               model=\"models/embedding-001\")\n",
        "\n",
        "db = Chroma.from_documents(text_chunks, embedding_model, persist_directory=\"./chroma_db_\")\n",
        "db.persist()\n",
        "\n",
        "\n",
        "db_connection = Chroma(persist_directory=\"./chroma_db_\", embedding_function=embedding_model)\n",
        "retriever = db_connection.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(google_api_key=\"____\",\n",
        "                             model=\"models/gemini-1.5-pro-latest\")\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessage(content=\"\"\"You are a Helpful AI Bot.\n",
        "    You take the context and question from user. Your answer should be based on the specific context.\"\"\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"\"\"Answer the question based on the given context.\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Answer: \"\"\")\n",
        "])\n",
        "\n",
        "rag_chain = RAGChain(retriever, llm, output_parser, format_docs)\n",
        "\n",
        "user_input = \"Can you explain the main contribution of the Leave No Context Behind paper?\"\n",
        "\n",
        "\n",
        "retrieved_docs = retriever.invoke(user_input)\n",
        "filtered_docs = [doc for doc in retrieved_docs if isinstance(doc, Document)]\n",
        "\n",
        "if filtered_docs:\n",
        "    response = rag_chain.invoke(user_input, retrieved_docs=filtered_docs)\n",
        "    print(response)\n",
        "else:\n",
        "    print(\"No valid Document objects found in retrieved_docs.\")\n",
        "\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s74v7oAVtHEn",
        "outputId": "5a5e3e6c-9a5c-4230-934b-e03b28ee98e2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No valid Document objects found in retrieved_docs.\n",
            "## Summary of \"Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention\"\n",
            "\n",
            "This research introduces **Infini-attention**, a novel method for scaling Transformer-based Large Language Models (LLMs) to handle infinitely long input sequences while maintaining manageable memory and computational requirements. \n",
            "\n",
            "**The Problem:** Traditional Transformer models struggle with long sequences due to the quadratic complexity of the attention mechanism, leading to massive memory consumption and computational costs. This limits their ability to process and understand extensive contexts effectively.\n",
            "\n",
            "**The Solution:** Infini-attention tackles this challenge by incorporating a **compressive memory** into the attention mechanism. This memory efficiently stores and retrieves information from long sequences using a fixed number of parameters, avoiding the memory explosion of standard attention. The method combines both **masked local attention** and **long-term linear attention** within a single Transformer block, allowing the model to capture both local and global context dependencies.\n",
            "\n",
            "**Benefits:**\n",
            "\n",
            "* **Efficiently handles infinitely long sequences:** Breaks the limitations of traditional attention mechanisms, enabling LLMs to process extensive contexts.\n",
            "* **Bounded memory and computation:** The compressive memory ensures manageable resource usage even with long sequences.\n",
            "* **Fast streaming inference:** Enables real-time processing of streaming data, making it suitable for various applications. \n",
            "\n",
            "**Effectiveness:** The paper demonstrates the effectiveness of Infini-attention on various long-context language modeling tasks, including:\n",
            "\n",
            "* **1M sequence length passkey context block retrieval:** Shows the ability to handle extremely long sequences effectively.\n",
            "* **500K length book summarization:** Demonstrates the capability to understand and summarize extensive contexts.\n",
            "\n",
            "**Overall, Infini-attention offers a promising approach for building more efficient and scalable LLMs capable of handling the ever-growing demand for processing and understanding long and complex sequences of information.** \n",
            "\n"
          ]
        }
      ]
    }
  ]
}